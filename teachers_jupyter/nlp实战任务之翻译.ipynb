{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9b48ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:17:06.721460Z",
     "start_time": "2023-02-28T13:17:03.872076Z"
    }
   },
   "outputs": [],
   "source": [
    "# hello -> 你好\n",
    "# seq -> seq\n",
    "# Transformer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19c81c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:23:02.361810Z",
     "start_time": "2023-02-28T13:17:36.711294Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd494fb75514c16bc0ce2c4de82fed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/source.spm:   0%|          | 0.00/789k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SupercoldZzz\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fd5e1306384ce1a4b34c90aa37de06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/target.spm:   0%|          | 0.00/817k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76f8e8360674a4680e9ab4f7ec7bb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarianTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-ro', vocab_size=59543, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\.venv\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-ro', use_fast=True)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9bd0961",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:24:26.528823Z",
     "start_time": "2023-02-28T13:24:26.505884Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[92, 778, 3, 1773, 879, 32, 8, 265, 431, 84, 32, 1450, 3, 709, 100, 540, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus([['hello, everyone today is a good day', 'It is late, please go home']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b21edfa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:51:18.078369Z",
     "start_time": "2023-02-28T13:27:02.780206Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e16b21172d4525ad89adeed286943e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f2a58a6859439d876e31c526c2cb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/18.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5e297f284945b8aac96928e3d91664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2cca6574f24f01add5198e687d00b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/41.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wmt16/ro-en to C:/Users/SupercoldZzz/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6948c2e9dba34550aba9e2b83bd02af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bd381a3c4b4edfafbdbe4f72278593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/225M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0996c4a0663946a28123a74670d40b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a509b52e8648b3bf540e3e9b48b99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/38.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66aebd51b04c4d66ab22b51ac7dc1f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b839586eac4ec9a6a47b2510297272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/610320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wmt16 downloaded and prepared to C:/Users/SupercoldZzz/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d92acba5754d7b925b37230550500e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(path='wmt16', name='ro-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3785939d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:52:53.469894Z",
     "start_time": "2023-02-28T13:52:53.456929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 610320\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a38159d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:53:41.608218Z",
     "start_time": "2023-02-28T13:53:41.433683Z"
    }
   },
   "outputs": [],
   "source": [
    "# 采样\n",
    "dataset['train'] = dataset['train'].shuffle(1).select(range(20000))\n",
    "dataset['validation'] = dataset['validation'].shuffle(1).select(range(200))\n",
    "dataset['test'] = dataset['test'].shuffle(1).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e02b28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:54:11.329769Z",
     "start_time": "2023-02-28T13:54:11.310819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': 'For these reasons I voted in favour of the proposal for a new regulation that aims for greater clarity and transparency in the GSP system.',\n",
       "  'ro': 'Din aceste motive am votat în favoarea propunerii de nou regulament care își propune o mai mare claritate și transparență în sistemul SPG.'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95318ecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:56:50.799495Z",
     "start_time": "2023-02-28T13:56:50.792514Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_function(data, tokenizer):\n",
    "    en = [ex['en'] for ex in data['translation']]\n",
    "    ro = [ex['ro'] for ex in data['translation']]\n",
    "    \n",
    "    data = tokenizer.batch_encode_plus(en, max_length=128, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        data['labels'] = tokenizer.batch_encode_plus(\n",
    "        ro, max_length=128, truncation=True)['input_ids']\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dfa2bc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:58:12.733480Z",
     "start_time": "2023-02-28T13:58:04.514450Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dc0f2d6cff442a88f4f0ec0a6f879f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3582: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  \"`as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc167b618024a1f8d693df54e8da214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2423bf227c1e48308fdab68ed84bac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(preprocess_function,\n",
    "           batched=True,\n",
    "           batch_size=1000,\n",
    "           num_proc=1,\n",
    "           remove_columns=['translation'],\n",
    "           fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "261d5351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T13:58:32.897580Z",
     "start_time": "2023-02-28T13:58:32.884614Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [460, 354, 3794, 12, 10677, 20, 5046, 14, 4, 2546, 37, 8, 397, 5551, 30, 10113, 37, 3501, 19814, 18, 8465, 20, 4, 44690, 782, 2, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [902, 576, 2946, 76, 10815, 17, 5098, 14997, 5, 559, 1140, 43, 2434, 6624, 27, 50, 337, 19216, 46, 22174, 17, 2317, 121, 16825, 2, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52e523ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:06:31.362630Z",
     "start_time": "2023-02-28T14:06:31.353655Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    # 求最长的label\n",
    "    max_length=max([len(i['labels']) for i in data])\n",
    "    \n",
    "    for i in data:\n",
    "        pads = [-100] * (max_length - len(i['labels']))\n",
    "        i['labels'] = i['labels'] + pads\n",
    "        \n",
    "    data = tokenizer.pad(\n",
    "        encoded_inputs=data,\n",
    "        padding=True,\n",
    "        max_length=None,\n",
    "        pad_to_multiple_of=None,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # decoder_input_ids\n",
    "    data['decoder_input_ids'] = torch.full_like(data['labels'], \n",
    "                                               tokenizer.get_vocab()['pad'],\n",
    "                                               dtype=torch.long)\n",
    "    data['decoder_input_ids'][:, 1:] = data['labels'][:, :-1]\n",
    "    data['decoder_input_ids'][data['decoder_input_ids'] == -100] = tokenizer.get_vocab()['<pad>']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "529106fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:07:46.534199Z",
     "start_time": "2023-02-28T14:07:46.502284Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset['train'],\n",
    "    batch_size=8,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "for data in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bf40e39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:07:49.913165Z",
     "start_time": "2023-02-28T14:07:49.891224Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   12,  1107,    30,    37,     4,  2194,   476,    63,   123,    47,\n",
       "           116,    15, 27384,  1036,     3,    18,    66,     8,  9911,  1591,\n",
       "           141,     2,     0, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542],\n",
       "        [   84,    32,   154,  2129,     8,  1348,    14,  2868,  6607,  1618,\n",
       "            13,   696,  5746,     2, 19106,     3,    18,   561,    12,    76,\n",
       "          4754,   108,     4,    15, 38405,  4997,    14,     4,   346,     7,\n",
       "            11,  1831,     3,   350,  5985,    32,   212,   320,   549,  3722,\n",
       "             4,  2656,  1093,    37,  7753,    20,     4,  9974,  7672,     7,\n",
       "             2,     0, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542],\n",
       "        [   68,  1585,  1367, 19886,  2855,   151,   873,    13,    64,   239,\n",
       "           623, 11324,     3,   202,    87,   434,     8,   927,  1651,  2508,\n",
       "             3,    18,   202,    64,     8,  6197,    37,     8,  2970,    51,\n",
       "           668,   217,  1092, 17147,    18,   668,   217,   654,    75, 15006,\n",
       "             2,     0, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542],\n",
       "        [ 4468,    99,  6119,     3,  6267,     3,     8,  5068, 27430,   322,\n",
       "           279,     3,  5116,   171,  1547,    13,     8, 15142,   105,  3475,\n",
       "         11880,    20, 15300,    81,  8259,   535,  2764,     2,     0, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542],\n",
       "        [   68, 17695,   186, 18771, 13485,   133,    95,     4, 27061,    75,\n",
       "            14,     4, 15933,   865,     4,  1674,     2,     0, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542],\n",
       "        [13663,    11,  2775,     8,  2680,  1299,   235, 25229,   492, 45153,\n",
       "          5279,  1084,     9,   492, 33491,    40, 23509, 33914, 12195,    37,\n",
       "         19615,     9,  7913, 22251,   340,   193, 19052,    11,     4,  3905,\n",
       "            14,  2492, 13777,   107, 27700,  1084,    18,   235, 25793,    40,\n",
       "         39400, 23296, 22250,    14,  7429,   340,  8081,   133,    51, 10001,\n",
       "             4,  2847,   107,    18,   235, 25229,   492, 45153,  5279,  1084,\n",
       "             9,  1030, 22009,   543,    40, 34525,  7774,  9145,    14, 18704,\n",
       "            11,    18,   162,  1114,  1771,  7122,   340,   193, 18938,    82,\n",
       "          7413,     7,    11,  1187,   756,    20,  7429,     2,     0],\n",
       "        [ 3024,   240,   174,     4,   234, 44129,  2040,   183,  1323,     6,\n",
       "             0, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542],\n",
       "        [  172,  2515,  1297,     3,    74,    64,  5023,   133, 23076,    18,\n",
       "          9000,    11, 17351, 21120,     2,     0, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[ 1939,    70,    39,  2149,  3042,   701,    19,   224,    27,  6461,\n",
       "          5968,  9188,    31,    29,   916, 11537,    49,  9803,    71,     2,\n",
       "             0,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100],\n",
       "        [  504,    29,    59,    50,  1121,  1720, 22401,   378,  2330,    50,\n",
       "          2169,    24, 13450,  1899,     3,   901,    50,  6349,     3,   901,\n",
       "          9026,    44,   559,    44, 11750,     5,  2946,     8, 21731,   730,\n",
       "             3,  2185,  3093,    55,     5,    48,    21,  4215,    22,  1756,\n",
       "           671,     9,  8953,    39,  5349,  4935,    44,  7175,  5925,     1,\n",
       "             2,     0,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100],\n",
       "        [23393,    62,    57,   369,     5, 20529,  2549,     5,    28, 12258,\n",
       "           396,    39,     8,     9,   396, 18858, 22599,     3,    43,   189,\n",
       "          7565,    42,  1165,    50,  1291,    17,  2935,    31,    43,   124,\n",
       "         22406,   411,  5349,  1920,    28,   219,    50,   455, 18296, 15539,\n",
       "            31,   219,    50,  1953,   268,    49,    43,    59,  6502,     5,\n",
       "          2228,     2,     0,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100],\n",
       "        [   91,  5234,  6119,     3,    20, 16544,    21,     5,  6267,     5,\n",
       "           343,     3,    79,  7675,   122,    12,  6677,   185,    62,   161,\n",
       "             3,   137,     9,    21,  4882,  6724,    62,    70,  3367,    27,\n",
       "         14720,  1981,    21,    20, 11464, 19761,    44,  2957,     2,     0,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100],\n",
       "        [42162,    72,   124,    79, 15776,  1549,    49,   258,     5,     5,\n",
       "         33047,    21, 18437,   161,  1206,     5,  1674,     2,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100],\n",
       "        [ 9771,    35,  3564,   671,    43,  5274,    42,  3878,    59, 10938,\n",
       "           235, 25229,   492, 45153,  5279,  1084,     9, 19146, 13593,    40,\n",
       "           543, 11297,    21,     5, 32065,   210,    39, 18637,  3381,  2788,\n",
       "           452,  3404,   340,    43, 25445,  9592,   162, 37173,     3, 27700,\n",
       "          1084,    31, 29799,   492,    40,   518,  1280,  9276,  2740,    39,\n",
       "         10751,  1236,   221,   122, 24105,   340, 24588,    28,  2983,   266,\n",
       "         26202,     3,    31,   235, 25229,   492, 45153,  5279,  1084,     9,\n",
       "          1030, 22009,   543,    40, 32940,    62,     5, 16711,  1355,    49,\n",
       "          2621,    17, 12499,  5162,    31,   162,   463,  1771,  5975,   340,\n",
       "            43,    59, 22603,  1337,  1669,     5,  1187,   122,  7413,    17,\n",
       "          9343,     2,     0],\n",
       "        [  109,   209,    55,  5752,  2317,  2212, 44129,     6,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100],\n",
       "        [  127,  5742,   343,     3,    76,    79, 27209, 40989,    46, 24725,\n",
       "           181,    43, 34119, 32121,     2,     0,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100]]), 'decoder_input_ids': tensor([[34426,  1939,    70,    39,  2149,  3042,   701,    19,   224,    27,\n",
       "          6461,  5968,  9188,    31,    29,   916, 11537,    49,  9803,    71,\n",
       "             2,     0, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542],\n",
       "        [34426,   504,    29,    59,    50,  1121,  1720, 22401,   378,  2330,\n",
       "            50,  2169,    24, 13450,  1899,     3,   901,    50,  6349,     3,\n",
       "           901,  9026,    44,   559,    44, 11750,     5,  2946,     8, 21731,\n",
       "           730,     3,  2185,  3093,    55,     5,    48,    21,  4215,    22,\n",
       "          1756,   671,     9,  8953,    39,  5349,  4935,    44,  7175,  5925,\n",
       "             1,     2,     0, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542],\n",
       "        [34426, 23393,    62,    57,   369,     5, 20529,  2549,     5,    28,\n",
       "         12258,   396,    39,     8,     9,   396, 18858, 22599,     3,    43,\n",
       "           189,  7565,    42,  1165,    50,  1291,    17,  2935,    31,    43,\n",
       "           124, 22406,   411,  5349,  1920,    28,   219,    50,   455, 18296,\n",
       "         15539,    31,   219,    50,  1953,   268,    49,    43,    59,  6502,\n",
       "             5,  2228,     2,     0, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542],\n",
       "        [34426,    91,  5234,  6119,     3,    20, 16544,    21,     5,  6267,\n",
       "             5,   343,     3,    79,  7675,   122,    12,  6677,   185,    62,\n",
       "           161,     3,   137,     9,    21,  4882,  6724,    62,    70,  3367,\n",
       "            27, 14720,  1981,    21,    20, 11464, 19761,    44,  2957,     2,\n",
       "             0, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542],\n",
       "        [34426, 42162,    72,   124,    79, 15776,  1549,    49,   258,     5,\n",
       "             5, 33047,    21, 18437,   161,  1206,     5,  1674,     2,     0,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542],\n",
       "        [34426,  9771,    35,  3564,   671,    43,  5274,    42,  3878,    59,\n",
       "         10938,   235, 25229,   492, 45153,  5279,  1084,     9, 19146, 13593,\n",
       "            40,   543, 11297,    21,     5, 32065,   210,    39, 18637,  3381,\n",
       "          2788,   452,  3404,   340,    43, 25445,  9592,   162, 37173,     3,\n",
       "         27700,  1084,    31, 29799,   492,    40,   518,  1280,  9276,  2740,\n",
       "            39, 10751,  1236,   221,   122, 24105,   340, 24588,    28,  2983,\n",
       "           266, 26202,     3,    31,   235, 25229,   492, 45153,  5279,  1084,\n",
       "             9,  1030, 22009,   543,    40, 32940,    62,     5, 16711,  1355,\n",
       "            49,  2621,    17, 12499,  5162,    31,   162,   463,  1771,  5975,\n",
       "           340,    43,    59, 22603,  1337,  1669,     5,  1187,   122,  7413,\n",
       "            17,  9343,     2],\n",
       "        [34426,   109,   209,    55,  5752,  2317,  2212, 44129,     6,     0,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542],\n",
       "        [34426,   127,  5742,   343,     3,    76,    79, 27209, 40989,    46,\n",
       "         24725,   181,    43, 34119, 32121,     2,     0, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542, 59542,\n",
       "         59542, 59542, 59542]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84ec1f4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:08:32.721734Z",
     "start_time": "2023-02-28T14:08:32.706775Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([8, 89])\n",
      "attention_mask torch.Size([8, 89])\n",
      "labels torch.Size([8, 103])\n",
      "decoder_input_ids torch.Size([8, 103])\n"
     ]
    }
   ],
   "source": [
    "for k, v in data.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6188516",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:09:33.115300Z",
     "start_time": "2023-02-28T14:09:33.050472Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, MarianModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5992c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:31:15.174840Z",
     "start_time": "2023-02-28T14:31:15.164867Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pretrained = MarianModel.from_pretrained('Helsinki-NLP/opus-mt-en-ro')\n",
    "        \n",
    "        self.register_buffer('final_logits_bias', torch.zeros(1, tokenizer.vocab_size))\n",
    "    \n",
    "        self.fc = torch.nn.Linear(512, tokenizer.vocab_size, bias=False)\n",
    "        \n",
    "        # 加载预训练权重参数\n",
    "        parameters = AutoModelForSeq2SeqLM.from_pretrained('Helsinki-NLP/opus-mt-en-ro')\n",
    "        self.fc.load_state_dict(parameters.lm_head.state_dict())\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels, decoder_input_ids):\n",
    "        logits = self.pretrained(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                decoder_input_ids=decoder_input_ids)\n",
    "        logits = logits.last_hidden_state\n",
    "        logits = self.fc(logits) + self.final_logits_bias\n",
    "        loss = self.criterion(logits.flatten(end_dim=1), labels.flatten())\n",
    "        return {'loss': loss, 'logits': logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "535413c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:31:27.674428Z",
     "start_time": "2023-02-28T14:31:18.461056Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Helsinki-NLP/opus-mt-en-ro were not used when initializing MarianModel: ['final_logits_bias']\n",
      "- This IS expected if you are initializing MarianModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MarianModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f0eacf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:16:30.363964Z",
     "start_time": "2023-02-28T14:16:30.353990Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105634816\n"
     ]
    }
   ],
   "source": [
    "print(sum(i.numel() for i in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "225a0c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:17:36.165073Z",
     "start_time": "2023-02-28T14:17:35.223589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4804006814956665, torch.Size([8, 103, 59543]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(**data)\n",
    "out['loss'], out['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6555cb80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:37:02.723818Z",
     "start_time": "2023-02-28T14:37:02.705867Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    loader_test = torch.utils.data.DataLoader(\n",
    "        dataset=dataset['test'],\n",
    "        batch_size=8,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for i, data in enumerate(loader_test):\n",
    "        with torch.no_grad():\n",
    "            out = model(**data)\n",
    "            \n",
    "        pred = tokenizer.batch_decode(out['logits'].argmax(dim=2))\n",
    "        label = tokenizer.batch_decode(data['decoder_input_ids'])\n",
    "        predictions.append(pred)\n",
    "        references.append(label)\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            print(i)\n",
    "            input_ids = tokenizer.decode(data['input_ids'][0])\n",
    "            print('input_ids=', input_ids)\n",
    "            print('pred=', pred[0])\n",
    "            print('label=', label[0])\n",
    "            \n",
    "        if i == 10:\n",
    "            break\n",
    "            \n",
    "    references = [[j] for j in references]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b07fde4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:21:53.052395Z",
     "start_time": "2023-02-28T14:21:46.437078Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "input_ids= The▁only name that▁was not▁mentioned by▁any of the▁participants in the▁negotiations of recent▁days is that of the▁former▁head of the▁branch,▁Mayor Gheorghe Nichita.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "pred= Singurul nume care nu a fost men de niciunul dintre participanții la negocierile din ultimele zile este cel al fostului șef al filialei, primarul Gheorghe Nichita.</s> DEaa al al Nicol Nicol Nicol N N În În În În În În În În În În În În În În În În În În Singur Singur Singur Singur Singur Singur\n",
      "label= pad Singurul nume care nu a fost menționat de niciunul din participanții la negocierile din ultimele zile este cel al fostului lider al filialei, primarul Gheorghe Nichita.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "2\n",
      "input_ids= I▁have not▁seen▁any▁USL (Social▁Liberal Union)▁projects from 2012 to continue.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "pred= N am văzut niciun din proiectele USL ( 2012 ( să continue.</s>,ul::----- În În În În În În În În În N N N N N N N N N N\n",
      "label= pad Nu am văzut nimic din proiectele USL din 2012 care să continue.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "4\n",
      "input_ids= The▁Labour▁leader,▁who▁has▁promised a▁different▁approach to▁politics,▁says he▁has \"crowd▁sourced\"▁ideas for▁questions to▁ask Mr▁Cameron and▁has▁been▁given▁more▁than 30,000▁suggestions.</s>\n",
      "pred= Liderul Munciiburist, care a promis o abordare diferită a politicii, declară că are idei \",din care comune\" pentru întrebări care care să i le pună d Cameron şi i i primit peste 30.000 de sugestii.</s> </s> ulul\n",
      "label= pad Liderul laburist, care a promis o abordare diferită a politicii, spune că are idei <unk> din surse externe<unk> pentru întrebări pe care să i le adreseze Domnului Cameron și că a primit peste 30.000 de sugestii.</s> <pad> <pad>\n",
      "6\n",
      "input_ids= The▁only▁thing I▁could▁see▁was the▁ball,▁which▁was▁heading in a▁different▁direction, so▁it▁looked▁like he [Moreno]▁touched▁it, but I▁couldn't▁see the▁tackle on▁Luke.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "pred= Singur vedea putut decât văd decât mingea, care se îndrepta într-o direcție direcție, așa că părea părut că [Moreno] a atins-o, nu am putut vedea plac lui lui Luke.</s>, - - - - Mi M M El El El El Mi Mi Mi Mi Singur Singur Singur Singur Singur Singur Singur Singur Singur Singur Singur Singur Singur Singur Singur Singur Singur\n",
      "label= pad Nu am reușit să văd decât mingea, care se îndrepta într-o altă direcție, așa că a părut că [Moreno] a atins-o însă nu am putut vedea intervenția asupra lui Luke.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "8\n",
      "input_ids= The pastor▁mentioned that Vadim Tudor▁used to▁come▁often to▁this▁church,▁especially in the▁evening,▁after 8 p.m., and▁ask▁him to▁open the reliquary with the relics of St. Nicholas. He▁would▁remain▁here and▁pray.</s> <pad> <pad> <pad> <pad> <pad>\n",
      "pred= Pasul avenit a menvazut sa- ca Vadim Tudor venea adeseas la biseric biserica, mai ales seara, dupa ora 8,00, si dup i ruga sa deschida rezboiul cu moastele Sffantului Nicolae, sămanea aici se roage aici intre urma,</s>\n",
      "label= pad Preotul paroh a tinut sa precizeze ca Vadim Tudor venea des în aceasta biserica, mai ales seara, dupa ora 20.00, iar atunci il ruga sa deschida racla cu moastele Sfantului Nicolae și ramanea sa se roage minute în sir.\n",
      "10\n",
      "input_ids= ▁That▁would▁point to a▁stock▁market▁drop▁if the Fed▁raises the rate,▁unless▁policymakers▁were to▁soften the▁blow by▁promising that▁another▁increase▁would be a▁ways▁off.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "pred= Acest ar indica o scădere a piața, federaled- rata,, cu cazul în care factorii care elaborează politici ar reduce lovitura,mițând că o exista o peste de la o creștere a</s>,, Acest În În În În În În În În În În Acest Acest Acest\n",
      "label= pad Aceasta ar indica o scădere pe bursă dacă Fed crește rata dobânzii, exceptând cazul în care cei care elaborează politicile ar atenua lovitura promițând că ar trece mult timp până la următoarea creștere.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "720ce5a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:22:43.582326Z",
     "start_time": "2023-02-28T14:22:43.514506Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e6b6261",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:24:19.032180Z",
     "start_time": "2023-02-28T14:24:18.835706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3d64daf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:31:53.331844Z",
     "start_time": "2023-02-28T14:31:53.323866Z"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = get_scheduler(name='linear',\n",
    "                             num_warmup_steps=0,\n",
    "                             num_training_steps=len(loader),\n",
    "                             optimizer=optimizer)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(loader):\n",
    "        for k in data.keys():\n",
    "            data[k] = data[k].to(device)\n",
    "        out = model(**data)\n",
    "        loss = out['loss']\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            out = out['logits'].argmax(dim=2)\n",
    "            correct = (data['decoder_input_ids'] == out).sum().item()\n",
    "            total = data['decoder_input_ids'].shape[1] * 8\n",
    "            accuracy = correct / total\n",
    "            \n",
    "            predictions = []\n",
    "            references = []\n",
    "            \n",
    "            for j in range(8):\n",
    "                pred = tokenizer.decode(out[j])\n",
    "                label = tokenizer.decode(data['decoder_input_ids'][j])\n",
    "                predictions.append(pred)\n",
    "                references.append(label)\n",
    "                \n",
    "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            print(i, loss.item(), accuracy, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "131966d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:36:05.423986Z",
     "start_time": "2023-02-28T14:31:55.397323Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.2159132957458496 0.0 1.9992e-05\n",
      "50 0.5485211610794067 0.0 1.9592e-05\n",
      "100 1.1590079069137573 0.006147540983606557 1.9192000000000002e-05\n",
      "150 0.9607174396514893 0.002777777777777778 1.8792000000000002e-05\n",
      "200 0.9061514735221863 0.0011261261261261261 1.8392e-05\n",
      "250 0.8414883613586426 0.0 1.7992e-05\n",
      "300 0.5579505562782288 0.0 1.7592000000000004e-05\n",
      "350 0.8098045587539673 0.01524390243902439 1.7192e-05\n",
      "400 0.7466010451316833 0.009259259259259259 1.6792e-05\n",
      "450 0.7285252809524536 0.006944444444444444 1.6392e-05\n",
      "500 0.7612788081169128 0.005208333333333333 1.5992000000000002e-05\n",
      "550 0.518362283706665 0.0023584905660377358 1.5592e-05\n",
      "600 0.6960635781288147 0.010080645161290322 1.5192000000000003e-05\n",
      "650 1.0835756063461304 0.005434782608695652 1.4792000000000002e-05\n",
      "700 0.8726363182067871 0.004464285714285714 1.4392000000000002e-05\n",
      "750 1.0110883712768555 0.004261363636363636 1.3992000000000001e-05\n",
      "800 0.6736987829208374 0.0030864197530864196 1.3592000000000001e-05\n",
      "850 1.0575217008590698 0.0018656716417910447 1.3192e-05\n",
      "900 0.9916527271270752 0.0014367816091954023 1.2792e-05\n",
      "950 0.7162908315658569 0.0025 1.2392000000000003e-05\n",
      "1000 0.6743871569633484 0.00641025641025641 1.1992000000000001e-05\n",
      "1050 1.0352495908737183 0.0025 1.1592000000000002e-05\n",
      "1100 1.1342477798461914 0.0033783783783783786 1.1192e-05\n",
      "1150 0.7612757682800293 0.0 1.0792000000000001e-05\n",
      "1200 1.0892364978790283 0.007352941176470588 1.0392e-05\n",
      "1250 0.5425900816917419 0.005319148936170213 9.992e-06\n",
      "1300 0.927254319190979 0.0055147058823529415 9.592e-06\n",
      "1350 0.8227509260177612 0.0037313432835820895 9.192000000000001e-06\n",
      "1400 0.5877779722213745 0.003472222222222222 8.792e-06\n",
      "1450 0.8488491177558899 0.003787878787878788 8.392e-06\n",
      "1500 0.8178256154060364 0.0 7.992e-06\n",
      "1550 0.9604809880256653 0.003968253968253968 7.592e-06\n",
      "1600 0.882363498210907 0.0014367816091954023 7.192e-06\n",
      "1650 0.8618381023406982 0.0030864197530864196 6.792000000000001e-06\n",
      "1700 1.0137814283370972 0.004545454545454545 6.392000000000001e-06\n",
      "1750 0.8783499002456665 0.007692307692307693 5.992e-06\n",
      "1800 0.8909816145896912 0.005208333333333333 5.592000000000001e-06\n",
      "1850 0.899616539478302 0.0 5.1920000000000004e-06\n",
      "1900 0.7367021441459656 0.00487012987012987 4.792000000000001e-06\n",
      "1950 0.5460206866264343 0.01020408163265306 4.3920000000000005e-06\n",
      "2000 0.6797389388084412 0.0 3.992e-06\n",
      "2050 0.9306153655052185 0.0021551724137931034 3.5920000000000005e-06\n",
      "2100 0.6148432493209839 0.00211864406779661 3.192e-06\n",
      "2150 0.7442500591278076 0.005681818181818182 2.792e-06\n",
      "2200 1.2125434875488281 0.0020833333333333333 2.392e-06\n",
      "2250 1.063299536705017 0.0025 1.992e-06\n",
      "2300 0.8368169665336609 0.007352941176470588 1.5920000000000002e-06\n",
      "2350 0.8302533626556396 0.001488095238095238 1.1920000000000002e-06\n",
      "2400 0.6920648217201233 0.006696428571428571 7.920000000000001e-07\n",
      "2450 0.8634450435638428 0.004032258064516129 3.92e-07\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24e14a82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:36:53.983185Z",
     "start_time": "2023-02-28T14:36:53.078600Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, '../data/翻译.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2044caf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:37:43.226552Z",
     "start_time": "2023-02-28T14:37:43.028081Z"
    }
   },
   "outputs": [],
   "source": [
    "model2 = torch.load('../data/翻译.model', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17ef8e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T14:37:53.049294Z",
     "start_time": "2023-02-28T14:37:46.168686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "input_ids= ▁Last▁month▁saw▁lowest▁growth▁rise▁since▁records▁began in 2000</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "pred= Luna ultima lună,-a înregistrat o mai mică creşte de 2000. în prezent.</s> țiululululul -: - De De De De De De De De De De De De De De De De Luna De De De De De De Luna De Luna Luna Luna Luna De Luna Luna De Luna Luna Luna Luna De Luna De Luna Luna Luna Luna Luna Luna Luna Luna\n",
      "label= pad În ultima lună s-a înregistrat cea mai lentă creștere din 2000 până în prezent.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "2\n",
      "input_ids= So far 20▁policemen▁were▁injured and the▁ambulance service▁took▁over▁two▁children▁who▁were▁injured▁after▁being▁thrown▁over the▁security▁fence.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "pred= Pâna acum 20 de politisti au fost raniti si serviciul de ambulanta a preluat doi copii care au fost raniti dupa ce au fost aruncati peste gardul de securitate.</s> - la A a Aul a a- a De\n",
      "label= pad Pana acum 20 de politisti au fost raniti și serviciul de ambulanta a preluat doi copii care au fost raniti dupa ce au fost aruncati peste gardul de securitate.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "4\n",
      "input_ids= Well,▁most▁people▁outside Ukip's▁woad-wearing▁tendency and the▁wilder▁shores of Tory▁little▁Englanderism▁can▁probably▁say▁yes to that.</s>\n",
      "pred= Ei oamenilor care nu se în acord cu tendtinut tendareaă din şi, şi putea spune de acord cu acest.</s>,, esteu este, Ei Ei Ei - Ei Ei Ei\n",
      "label= pad Majoritatea celor care nu sunt de acord cu <unk> anglicismul<unk> conservatorilor ar putea fi de acord cu asta.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "6\n",
      "input_ids= ▁Addressing the▁issue of▁violence, the Dalai Lama▁also▁commented on George Bush's▁actions▁following 9/11▁terrorist▁attacks,▁claiming that the▁US\" violent▁response engendered a▁chain of uncontrollable▁events.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "pred= Luând la problema violen, Dalai Lama a comentat de asemenea acțiunile acțiunile lui George Bush în atacurile teroriste din la 11 septembrie, afirmând că răspunsul violent al SUA SUA a generat un lanț de evenimente incontrolabile.</s> </s> :  A La Da A A - La  La La În În În La\n",
      "label= pad Referitor la problema violenței, Dalai Lama a comentat de asemenea despre acțiunile lui George Bush după atacurile teroriste de pe 11 septembrie, afirmând că răspunsul violent din partea SUA a generat un lanț de evenimente incontrolabile.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "8\n",
      "input_ids= Hogg▁was▁granted▁bail in▁this case but▁was▁held in▁custody▁ahead of▁hearings on▁other▁outstanding▁cases.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "pred= Îngg a fost eliberat pe cauţiuțiune în acest caz, a fost în custodie înaintea audieriștițișării altor alte cazuri nerezolvat</s> CEți.ae.. Ho În În Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho Ho\n",
      "label= pad Hogg a fost eliberat pe cauțiune în acest dosar însă a rămas în custodie înaintea înfățișărilor pentru alte dosare.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "10\n",
      "input_ids= Corneliu Vadim Tudor▁was▁born on▁November 28,▁1949, in▁Bucharest. He▁was a▁writer,▁politician and▁journalist.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "pred= Corneliu Vadim Tudor s-a născutscut in 28 noiembrie 1949, la Bucuresti. a scriitor, politician si jurnalist.</s> </s> al alul al al,  La al A A A I La Cor A Cor A Cor Cor Cor La Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Cor Corneliu Cor Cor Cor Cor Cor Cor Cor Cor\n",
      "label= pad Corneliu Vadim Tudor s-a nascut în 28 noiembrie 1949, în Bucuresti, era scriitor, politician și jurnalist.</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "test(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7274f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
