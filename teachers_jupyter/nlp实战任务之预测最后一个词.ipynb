{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb66e125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:19:11.618309Z",
     "start_time": "2023-02-20T12:19:08.788876Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f03cc2a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:23:05.278941Z",
     "start_time": "2023-02-20T12:23:00.707172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[24717, 649, 3200, 507, 422, 262, 21694, 4991], [5661, 6941, 425, 318, 1049]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2', use_fast=True)\n",
    "tokenizer.batch_encode_plus([\n",
    "    'hide new secretions from the parental units',\n",
    "    'this moive is great'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8109bb61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:23:22.932249Z",
     "start_time": "2023-02-20T12:23:22.927262Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c47369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测最后一个词 实际上是一个多分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b987bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:26:03.204373Z",
     "start_time": "2023-02-20T12:25:56.868322Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/SupercoldZzz/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536ce2406a664f2184c5dfdbf393a1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(path='glue', name='sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8285bed7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:28:33.881303Z",
     "start_time": "2023-02-20T12:28:33.865261Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a55f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:34:52.793883Z",
     "start_time": "2023-02-20T12:34:29.838150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    "
     ]
    }
   ],
   "source": [
    "def f(data, tokenizer):\n",
    "    return tokenizer.batch_encode_plus(data['sentence'])\n",
    "\n",
    "\n",
    "dataset = dataset.map(f, batched=True, batch_size=1000, num_proc=12, \n",
    "                     remove_columns=['sentence', 'idx', 'label'], \n",
    "                     fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e144f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:35:10.563978Z",
     "start_time": "2023-02-20T12:35:10.547938Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a36355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:37:35.116544Z",
     "start_time": "2023-02-20T12:37:26.622802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    "
     ]
    }
   ],
   "source": [
    "# 规定一个句子最少要有8个单词. \n",
    "# 过滤掉太短的句子\n",
    "def f(data):\n",
    "    return [len(i) >= 8 for i in data['input_ids']]\n",
    "\n",
    "dataset = dataset.filter(f, batched=True, batch_size=1000, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c8c461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:37:38.542380Z",
     "start_time": "2023-02-20T12:37:38.525425Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 39905\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 848\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1730\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fbde696",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:42:53.581009Z",
     "start_time": "2023-02-20T12:42:44.710040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    "
     ]
    }
   ],
   "source": [
    "# 截断句子\n",
    "def f(data):\n",
    "    data['input_ids'] = [i[:8] for i in data['input_ids']]\n",
    "    data['attention_mask'] = [[1] * 8] * len(data['attention_mask'])\n",
    "    # 模型帮我们做了偏移量问题, 这里输入和输出保持一致即可. \n",
    "    data['labels'] = data['input_ids']\n",
    "    return data\n",
    "\n",
    "dataset = dataset.map(f, batched=True, batch_size=1000, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65f3b2b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:43:04.096926Z",
     "start_time": "2023-02-20T12:43:04.085955Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 39905\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 848\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1730\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebbde5a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:43:14.847694Z",
     "start_time": "2023-02-20T12:43:14.834729Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [24717, 649, 3200, 507, 422, 262, 21694, 4991],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [24717, 649, 3200, 507, 422, 262, 21694, 4991]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43808a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:46:34.086991Z",
     "start_time": "2023-02-20T12:46:32.641816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2494,\n",
       " {'input_ids': tensor([[   64,  1643,   286,   257,   866,   263,   290,   257],\n",
       "          [ 6738,   923,   284,  5461,   837,  9593,   257,  2121],\n",
       "          [28895,   326,   262,   308,  2224,  2646,  2831,   460],\n",
       "          [ 1169,  7110,   523, 34377,   326,   772,   355, 23586],\n",
       "          [   66,  7749,  1512, 44624,   837,   257, 20278, 47602],\n",
       "          [ 3919,   837,   772,   326,   705,    82,  1165,  5364],\n",
       "          [   82,  7126, 29658,  3435,   290, 40620,   899, 20968],\n",
       "          [ 6738,   281, 39770,  1862,  7401,   508,  7228,   465],\n",
       "          [ 6738,   257,   458,  5088,  4420,  7758,   375,   859],\n",
       "          [ 5562,   837,   772,   287,   477,   663,  3437,   705],\n",
       "          [16275,  1771,   340,  3382,   284,   307,   257, 43527],\n",
       "          [  986,   257,   629, 13513, 18344,  2364,   286, 21970],\n",
       "          [26024,   588,   281,  7083, 10721,  5517,   287, 42964],\n",
       "          [44944,    12,   448,    12,    75,  2778, 14678,  1590],\n",
       "          [34751,  1363,   435,  8809,   318,   299,   470,  1016],\n",
       "          [43395,    13,  4636,  2433,   837,   257, 28892,  3437]]),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       "  'labels': tensor([[   64,  1643,   286,   257,   866,   263,   290,   257],\n",
       "          [ 6738,   923,   284,  5461,   837,  9593,   257,  2121],\n",
       "          [28895,   326,   262,   308,  2224,  2646,  2831,   460],\n",
       "          [ 1169,  7110,   523, 34377,   326,   772,   355, 23586],\n",
       "          [   66,  7749,  1512, 44624,   837,   257, 20278, 47602],\n",
       "          [ 3919,   837,   772,   326,   705,    82,  1165,  5364],\n",
       "          [   82,  7126, 29658,  3435,   290, 40620,   899, 20968],\n",
       "          [ 6738,   281, 39770,  1862,  7401,   508,  7228,   465],\n",
       "          [ 6738,   257,   458,  5088,  4420,  7758,   375,   859],\n",
       "          [ 5562,   837,   772,   287,   477,   663,  3437,   705],\n",
       "          [16275,  1771,   340,  3382,   284,   307,   257, 43527],\n",
       "          [  986,   257,   629, 13513, 18344,  2364,   286, 21970],\n",
       "          [26024,   588,   281,  7083, 10721,  5517,   287, 42964],\n",
       "          [44944,    12,   448,    12,    75,  2778, 14678,  1590],\n",
       "          [34751,  1363,   435,  8809,   318,   299,   470,  1016],\n",
       "          [43395,    13,  4636,  2433,   837,   257, 28892,  3437]])})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义数据加载器\n",
    "import torch\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset['train'],\n",
    "    batch_size=16,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "for data in loader:\n",
    "    break\n",
    "    \n",
    "len(loader), data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de61f1c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:47:44.241605Z",
     "start_time": "2023-02-20T12:47:44.171790Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5eaf56d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T12:50:34.252699Z",
     "start_time": "2023-02-20T12:50:34.238737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38597376"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "866dbf0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:04:28.849025Z",
     "start_time": "2023-02-20T13:04:23.701287Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilgpt2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12050.9952\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 简单写法 model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "        self.pretrained = GPT2Model.from_pretrained('distilgpt2')\n",
    "        self.fc = torch.nn.Linear(768, tokenizer.vocab_size, bias=False)\n",
    "        \n",
    "        # 给fc这一层加载预训练权重\n",
    "        parameters = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "        self.fc.load_state_dict(parameters.lm_head.state_dict())\n",
    "        \n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        logits = self.pretrained(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = logits.last_hidden_state\n",
    "        logits = self.fc(logits)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[:, :-1].reshape(-1, tokenizer.vocab_size)\n",
    "            shift_labels = labels[:, 1:].reshape(-1)\n",
    "            loss = self.criterion(shift_logits, shift_labels)\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "    \n",
    "model = Model()\n",
    "# 参数量\n",
    "print(sum(i.numel() for i in model.parameters()) / 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1af0346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:04:30.628309Z",
     "start_time": "2023-02-20T13:04:30.458719Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.5038, grad_fn=<NllLossBackward0>) torch.Size([16, 8, 50257])\n"
     ]
    }
   ],
   "source": [
    "# python中** 的用法, 可以自动把一个字典解包成关键词参数{} -> xxx =xxx, xxx=xxx\n",
    "out = model(**data)\n",
    "print(out['loss'], out['logits'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4f301a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:43:23.610475Z",
     "start_time": "2023-02-20T13:43:23.602496Z"
    }
   },
   "outputs": [],
   "source": [
    "# 测试代码\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    \n",
    "    # 加载测试数据\n",
    "    loader_test = torch.utils.data.DataLoader(\n",
    "        dataset=dataset['test'],\n",
    "        batch_size=16,\n",
    "        collate_fn=default_data_collator,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(loader_test):\n",
    "        # 只计算最后一个词的准确率. \n",
    "        label = data['input_ids'][:, -1].clone()\n",
    "        # 再从数据中抹除最后一个词, 防止模型左闭. \n",
    "        data['input_ids'][:, -1] = 0\n",
    "        # label就不需要了\n",
    "        data['labels'][:, :]  = 0\n",
    "        \n",
    "        # 计算\n",
    "        with torch.no_grad():\n",
    "            out = model(**data)\n",
    "            \n",
    "        # 最后一个词的准确率, 因为有偏移量的关系, 这里取的是倒数第二个词\n",
    "        out = out['logits'].argmax(dim=2)[:, -2]\n",
    "        correct += (label==out).sum().item()\n",
    "        total += 16\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "            print(label)\n",
    "            print(out)\n",
    "            \n",
    "        if i == 50:\n",
    "            break\n",
    "            \n",
    "    print('accuracy: ', correct / total)\n",
    "    \n",
    "    for i in range(8):\n",
    "        print(tokenizer.decode(data['input_ids'][i, :-1]))\n",
    "        print(tokenizer.decode(label[i]), tokenizer.decode(out[i]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b0dfa37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:17:38.003317Z",
     "start_time": "2023-02-20T13:17:29.157309Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([ 3146, 11982,   264,   764, 39769,  2646,   764,   290,   705,  1502,\n",
      "          511, 10997,  2644,   290,   906,   287])\n",
      "tensor([ 976,   13, 3435,   13,   11, 1907,   13,  290,  821,  262,  484,  680,\n",
      "          13,  290,  561,   13])\n",
      "10\n",
      "tensor([  262,  3807,  1894, 21430,   355,   587,  2700,  3504,  1057, 20307,\n",
      "         2085,   764,   517,   290,  7770,   282])\n",
      "tensor([  262,  2854,  1894, 11170,   355,   587,  2700,   262,   257,    12,\n",
      "         1256,    13,   475,    11,   582, 40881])\n",
      "20\n",
      "tensor([ 1787,   284,   922,  1838,  2644,  4260,   837,   621, 12838,   467,\n",
      "          329,  4035, 19813,   290, 23251,   257])\n",
      "tensor([  76,  284,  257,  318,   13,  898,   13,  621,   12,  340,  329, 4035,\n",
      "         510,  292, 1943,  262])\n",
      "30\n",
      "tensor([  12, 1964, 5739, 1419,  257, 6096, 4952, 2223,  530,  837, 5023,  318,\n",
      "          82,  284, 3931,  640])\n",
      "tensor([  12,  262,  286, 4730,  262, 7328,   13,   11,  257,   11,  287,  318,\n",
      "          82,    6,  614,  640])\n",
      "40\n",
      "tensor([  938, 11331, 19798,  1912,  6958, 32799,   319,  1573,   481,  2462,\n",
      "         5989,   262, 10438,   636,  3573,  1107])\n",
      "tensor([ 2589,  1204,  5679,    11,   262,   774,    11,     6,    11,    11,\n",
      "          257,   262,   447,   257, 43527,   257])\n",
      "50\n",
      "tensor([  287,  9137,   326, 12121,   287,  2429,   546,   705,   262,  1936,\n",
      "          764,   326,  2565,   288, 23365,   284])\n",
      "tensor([  287,  1169,   326,   290,    11,    12,   546,    13,   262,   584,\n",
      "           13,   546,  3092,   976, 23365,   284])\n",
      "accuracy:  0.2034313725490196\n",
      "soul is what's lacking\n",
      " in  in\n",
      "\n",
      "the long-range appeal of ``\n",
      " minority the\n",
      "\n",
      "formula 51 is so trite\n",
      " that  that\n",
      "\n",
      "a worthy entry into a very difficult\n",
      " genre  and\n",
      "\n",
      "there is not an ounce of honesty\n",
      " in ,\n",
      "\n",
      "it extends the writings of jean\n",
      " gen -\n",
      "\n",
      "` dragonfly'is a movie\n",
      " about  about\n",
      "\n",
      "but based on cq, i\n",
      " ' .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 没有经过我们的数据训练的模型, 只是用了预训练的权重, 就达到了20%的准确率. \n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7be8612e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:20:08.060957Z",
     "start_time": "2023-02-20T13:20:08.010093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "170af58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:21:24.514193Z",
     "start_time": "2023-02-20T13:21:24.490256Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20ffcb73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:33:42.651893Z",
     "start_time": "2023-02-20T13:33:42.643914Z"
    }
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train():\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = get_scheduler(name='linear',\n",
    "                              num_warmup_steps=0,\n",
    "                              num_training_steps=len(loader),\n",
    "                              optimizer=optimizer)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for i, data in enumerate(loader):\n",
    "        input_ids, attention_mask, labels = data['input_ids'], data['attention_mask'], data['labels']\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = out['loss']\n",
    "        \n",
    "        loss.backward()\n",
    "        # 为了训练稳定, 进行梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            labels = labels[:, 1:]\n",
    "            out = out['logits'].argmax(dim=2)[:, :-1]\n",
    "            correct = (labels == out).sum().item()\n",
    "            accuracy = correct / (16 * 7)\n",
    "            \n",
    "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            \n",
    "            print(i, loss.item(), accuracy, lr)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9310cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:36:28.445416Z",
     "start_time": "2023-02-20T13:33:51.416445Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\.venv\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.916006565093994 0.21428571428571427 1.9991980753809144e-05\n",
      "50 5.455381870269775 0.20535714285714285 1.959101844426624e-05\n",
      "100 5.02870512008667 0.24107142857142858 1.919005613472334e-05\n",
      "150 4.951569557189941 0.22321428571428573 1.8789093825180436e-05\n",
      "200 5.210883617401123 0.21428571428571427 1.838813151563753e-05\n",
      "250 5.392312526702881 0.21428571428571427 1.7987169206094627e-05\n",
      "300 5.1180195808410645 0.23214285714285715 1.7586206896551724e-05\n",
      "350 5.2933030128479 0.17857142857142858 1.718524458700882e-05\n",
      "400 5.1162614822387695 0.21428571428571427 1.678428227746592e-05\n",
      "450 5.150652885437012 0.19642857142857142 1.6383319967923016e-05\n",
      "500 4.773595333099365 0.26785714285714285 1.5982357658380113e-05\n",
      "550 5.292885780334473 0.1875 1.558139534883721e-05\n",
      "600 4.781903266906738 0.21428571428571427 1.5180433039294307e-05\n",
      "650 4.8646240234375 0.20535714285714285 1.4779470729751404e-05\n",
      "700 4.640826225280762 0.21428571428571427 1.4378508420208502e-05\n",
      "750 4.749022960662842 0.2767857142857143 1.3977546110665599e-05\n",
      "800 4.62315034866333 0.30357142857142855 1.3576583801122696e-05\n",
      "850 4.936435222625732 0.25 1.3175621491579793e-05\n",
      "900 4.977120399475098 0.22321428571428573 1.277465918203689e-05\n",
      "950 4.980513572692871 0.20535714285714285 1.2373696872493987e-05\n",
      "1000 4.904933452606201 0.22321428571428573 1.1972734562951083e-05\n",
      "1050 4.638516902923584 0.2767857142857143 1.157177225340818e-05\n",
      "1100 5.132450103759766 0.22321428571428573 1.1170809943865277e-05\n",
      "1150 4.297675609588623 0.25 1.0769847634322374e-05\n",
      "1200 4.421633720397949 0.23214285714285715 1.0368885324779472e-05\n",
      "1250 4.6649298667907715 0.24107142857142858 9.967923015236569e-06\n",
      "1300 4.632845878601074 0.23214285714285715 9.566960705693666e-06\n",
      "1350 4.8495330810546875 0.21428571428571427 9.165998396150763e-06\n",
      "1400 4.33595609664917 0.2857142857142857 8.76503608660786e-06\n",
      "1450 4.8500189781188965 0.23214285714285715 8.364073777064956e-06\n",
      "1500 4.675201416015625 0.2767857142857143 7.963111467522053e-06\n",
      "1550 5.042179584503174 0.23214285714285715 7.562149157979151e-06\n",
      "1600 4.844080448150635 0.29464285714285715 7.161186848436248e-06\n",
      "1650 4.345889568328857 0.32142857142857145 6.7602245388933444e-06\n",
      "1700 4.674899578094482 0.25 6.359262229350442e-06\n",
      "1750 4.567580223083496 0.26785714285714285 5.958299919807539e-06\n",
      "1800 4.9128031730651855 0.20535714285714285 5.557337610264636e-06\n",
      "1850 5.346706867218018 0.20535714285714285 5.156375300721732e-06\n",
      "1900 5.015437602996826 0.19642857142857142 4.7554129911788294e-06\n",
      "1950 4.807806968688965 0.25892857142857145 4.354450681635927e-06\n",
      "2000 4.190169811248779 0.2767857142857143 3.953488372093024e-06\n",
      "2050 4.524538993835449 0.2857142857142857 3.5525260625501205e-06\n",
      "2100 4.734208106994629 0.22321428571428573 3.1515637530072173e-06\n",
      "2150 4.534892559051514 0.25892857142857145 2.7506014434643145e-06\n",
      "2200 4.371130466461182 0.2767857142857143 2.3496391339214116e-06\n",
      "2250 4.356747627258301 0.26785714285714285 1.948676824378509e-06\n",
      "2300 4.346745014190674 0.33035714285714285 1.5477145148356058e-06\n",
      "2350 4.84682559967041 0.19642857142857142 1.1467522052927025e-06\n",
      "2400 5.163261413574219 0.23214285714285715 7.457898957497996e-07\n",
      "2450 4.403899669647217 0.2767857142857143 3.4482758620689656e-07\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3bd8cae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:42:02.833209Z",
     "start_time": "2023-02-20T13:42:01.790996Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, '../data/预测最后一个词.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6298b373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:43:06.400958Z",
     "start_time": "2023-02-20T13:43:06.208472Z"
    }
   },
   "outputs": [],
   "source": [
    "model2 = torch.load('../data/预测最后一个词.model', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "646fb648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-20T13:43:42.027299Z",
     "start_time": "2023-02-20T13:43:33.001352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([ 6232,   379,    82,   532,  6958,  4035, 16223,  5321,   837,  2099,\n",
      "          318,   286,  5701,  3729, 43527, 10378])\n",
      "tensor([  262,   379,    82,    12,   257,  4035, 13289,  5321,  2003,  1611,\n",
      "          318,   284, 10997,   257,   262,  6042])\n",
      "10\n",
      "tensor([ 1033,  3210,   557,   262,  8824, 33471,  5280,   837,  3807,   290,\n",
      "         1107,   881,   764,   837, 11783,  2071])\n",
      "tensor([  326,  3807, 32745,   340,   787,  6490,   705,   837, 10997,   837,\n",
      "          257,   881,   220,   837,  3807, 10059])\n",
      "20\n",
      "tensor([  616,  3155,    12,   913,   290,   329,   287,    82, 11331, 18098,\n",
      "        12850, 28294,   416,  1645,  2589,  4600])\n",
      "tensor([  262,  3807,   288,   913,   290,   329,   220,    82,  2116, 10647,\n",
      "          290,   257,   416,   326,  1218,   290])\n",
      "30\n",
      "tensor([   64,   286,  7463, 36207, 12986, 15827,   764,   257,  5794,  1377,\n",
      "          290,  5739,    12,   262,   467,   257])\n",
      "tensor([ 7916,   837,   257, 36207,   262, 24953,   837,   588, 35478,   329,\n",
      "          220,   584,    12,   428,   257,   262])\n",
      "40\n",
      "tensor([  837, 19972,  3807,   262,    82,   340,   286, 10925,  2646,  1115,\n",
      "        12661,  1108,   906,  2673,  6022,   290])\n",
      "tensor([ 290, 3807, 1048,  257,   82,  340,  286,  389, 1109,  262, 2646, 1108,\n",
      "        1244, 2673,  377,  290])\n",
      "50\n",
      "tensor([  358, 32044,   703,   303,   581,   477,   477,   407,   290, 13437,\n",
      "         8925,  3296,  1628,   706,  5688,   286])\n",
      "tensor([  358, 32044,   262,   303,  1621,   262,  2642,   290,   475, 46374,\n",
      "         2568,   220,   837,   329,  2646,   284])\n",
      "accuracy:  0.28799019607843135\n",
      "legendary irish writer bre\n",
      "nd nd\n",
      "\n",
      "if you can get past the fant\n",
      "astical astical\n",
      "\n",
      "enough similarities to gymkata and\n",
      " how  the\n",
      "\n",
      "zany, exuberantly irre\n",
      "ve ve\n",
      "\n",
      "makmalbaf follows a\n",
      " res  story\n",
      "\n",
      "you have to pay attention to follow\n",
      " all  the\n",
      "\n",
      "only an epic documentary could get it\n",
      " all  wrong\n",
      "\n",
      "bad company leaves a bad taste,\n",
      " not  and\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
